{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Chinkara 7B (Improved)\n",
        "#@markdown This is the first model from [MaralGPT](https://huggingface.co/MaralGPT) project. It's an effort in making a free/libre and open source software (FLOSS) compatible Large Language Model (LLM). This model can be ran on only 8 GB of GPU VRAM and it's based on _Meta's LLaMa-2_."
      ],
      "metadata": {
        "cellView": "form",
        "id": "RFiEGoe2cdaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u50lQBC3PsaQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Installing Libraries\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Setting Up the Model and Improts\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"Trelis/Llama-2-7b-chat-hf-sharded-bf16\"\n",
        "adapters_name = 'MaralGPT/chinkara-7b-improved'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    ),\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2oRJCIxrP6O7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Setting up the model and QLoRa adapter\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Rwm4V9rkQTJr",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Running the inference\n",
        "#@markdown Mind that this may take some time and since the model is quantized, there is a chance of huge hallucinations.\n",
        "\n",
        "prompt = \"Who was the president of the united states in 1996?\" #@param{type: \"string\"}\n",
        "\n",
        "prompt = f\"### Human: {prompt} ### Assistant:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=100, temperature=0.75, repetition_penalty=1.2)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "answer = answer.split(\"### Assistant:\")[1]\n",
        "\n",
        "if \"### Human\" in answer:\n",
        "  answer = answer.split(\"### Human:\")[0]\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkAjcYxIRsaA",
        "outputId": "b85f5b38-f2b5-4448-e300-1ed505a15a32",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The President of the United States in 1996 was Bill Clinton. Unterscheidung between \"Bill Clinton\" and \"William Clinton\":\n",
            "* \"Bill Clinton\" refers to William Jefferson Clinton, who served as the 42nd President of the United States from 1993 to 2001.\n",
            "* \"William Clinton\" is an alternative spelling for the name of the same person, but it is less common than\n"
          ]
        }
      ]
    }
  ]
}