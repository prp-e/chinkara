# Chinkara 7B

_Chinkara_ is a Large Language Model trained on [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset based on Meta's brand new LLaMa-2 with 7 billion parameters using QLoRa Technique, optimized for small consumer size GPUs. 

## Inference Notebooks 

| Model | Notebook | Description |
|:-----:|:--------:|:------------:|
|[chinkara-7b](https://huggingdace.com/MaralGPT/chinkara-7b) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/prp-e/chinkara/blob/main/inference-7b.ipynb) | This is the smallest model of the family, trained on LLaMa-2 7B |