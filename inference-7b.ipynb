{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "u50lQBC3PsaQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Installing Libraries\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Setting Up the Model and Improts\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
        "adapters_name = 'MaralGPT/chinkara-7b'\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    ),\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2oRJCIxrP6O7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Setting up the model and QLoRa adapter\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Rwm4V9rkQTJr",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the answer to life, universe and everything?\" #@param{type: \"string\"}\n",
        "\n",
        "prompt = f\"###Human: {prompt} ###Assistant:\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "Any0OW-oRxAj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Running the inference\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=50, temperature=0.5, repetition_penalty=1.0)\n",
        "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "answer = answer.split(\"###Assistant:\")[1]\n",
        "\n",
        "if \"###Human\" in answer:\n",
        "  answer = answer.split(\"###Human:\")[0]\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "kkAjcYxIRsaA",
        "outputId": "fc3df460-4d0d-4009-ac9b-def3a0d0e7fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 42 \n"
          ]
        }
      ]
    }
  ]
}